{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA (Analysis of Variance) makes several key assumptions about the data being analyzed. These assumptions are crucial for the validity of the test results. Here's a breakdown of the assumptions and potential consequences of violating them:\n",
    "\n",
    "# 1. Normality:\n",
    "\n",
    "# Assumption: The data within each group (defined by the independent variable) should be normally distributed.\n",
    "# Violation Impact: If the data is significantly skewed or has outliers, the F-statistic used in ANOVA might not accurately reflect the true differences between groups. The p-value might be inflated or deflated, leading to misinterpretations.\n",
    "# Example: Imagine comparing plant growth across three fertilizer types. If one fertilizer consistently produces abnormally large plants (positive skew), ANOVA might overestimate the true difference between groups.\n",
    "\n",
    "# 2. Homogeneity of Variance:\n",
    "\n",
    "# Assumption: The variance (spread) of the data should be approximately equal across all groups.\n",
    "# Violation Impact: Unequal variances can make it difficult to interpret the F-statistic. Even if there are true group differences in means, the group with the higher variance might dominate the F-statistic, leading to misleading results.\n",
    "# Example: Let's say you're studying the effect of exercise duration on weight loss. If one group exercised for much longer periods (higher variance), the larger spread in their weight loss data might mask the true effect of exercise duration observed in the other groups.\n",
    "\n",
    "# 3. Independence:\n",
    "\n",
    "# Assumption: The observations within each group should be independent of each other. This means the outcome of one observation shouldn't influence the outcome of another.\n",
    "# Violation Impact: Dependence between observations can inflate the F-statistic and lead to falsely significant results.\n",
    "# Example: Imagine measuring student test scores before and after a teaching intervention. If students were grouped based on friendship (not random assignment), their scores might be correlated, violating independence. This could lead to an inflated F-statistic suggesting a stronger intervention effect than actually exists.\n",
    "\n",
    "# 4. Random Sampling:\n",
    "\n",
    "# Assumption: The data should be collected from a random sample of the population of interest.\n",
    "# Violation Impact: If the data is not randomly sampled, the results might not be generalizable to the entire population. Selection bias can lead to skewed results that don't reflect the true population trends.\n",
    "# Example:  A survey on social media usage is sent out to a specific group (e.g., gamers). This is not a random sample of the population, and the results might not reflect the social media habits of the general population.\n",
    "\n",
    "# Addressing Violations:\n",
    "\n",
    "# Transformations: Data transformations (e.g., log transformation) can sometimes help achieve normality.\n",
    "# Non-parametric Tests: If the assumptions are seriously violated, consider using non-parametric tests (e.g., Kruskal-Wallis test) that are less reliant on these assumptions.\n",
    "# Robust ANOVA Tests: Some ANOVA variations are more robust to violations of normality and homogeneity of variance.\n",
    "# It's important to check the assumptions of ANOVA before interpreting the results. By understanding these assumptions and their potential consequences, you can make informed decisions about the validity of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77521dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Way ANOVA:\n",
    "\n",
    "# Situation: This is the simplest type of ANOVA used when you have one independent variable (factor) with three or more levels (groups) and a continuous dependent variable (outcome variable). expand_more\n",
    "# Example: You want to compare the effectiveness of three different fertilizers on plant growth (three levels of the factor \"fertilizer type\") and measure the final plant height (continuous dependent variable).\n",
    "# Two-Way ANOVA:\n",
    "\n",
    "# Situation: This type of ANOVA is used when you have two independent variables (factors), each with two or more levels, and a continuous dependent variable. It allows you to analyze the main effects of each factor individually and the interaction effect between the two factors.expand_more\n",
    "# Example: You want to investigate the combined effects of exercise duration (two levels: short vs. long) and diet type (two levels: low-carb vs. high-carb) on weight loss (continuous dependent variable). You're interested in both the main effects of exercise and diet, as well as whether their combination has an additional impact on weight loss (interaction effect).\n",
    "# N-Way ANOVA (or Factorial ANOVA):\n",
    "\n",
    "# Situation: This is an extension of the two-way ANOVA for studies with more than two independent variables, each with multiple levels. It allows for examining the main effects of each factor and all possible interaction effects between them.\n",
    "# Example: A researcher is studying the effects of fertilizer type (three levels), watering frequency (two levels), and sunlight exposure (two levels) on plant growth (continuous dependent variable). This N-way ANOVA can analyze the main effects of each factor and all possible interactions (e.g., fertilizer type x watering frequency interaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e394b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105034b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning of variance is a fundamental concept in ANOVA (Analysis of Variance) that helps us understand how the total variation in the data is distributed across different sources. It's crucial because it allows us to assess the impact of the independent variable(s) on the dependent variable.\n",
    "\n",
    "# Here's a breakdown of the concept:\n",
    "\n",
    "# Total Variance: This represents the total variability observed in the dependent variable for all the data points. It reflects the combined effects of the independent variable(s), random error, and any other unknown factors influencing the data.\n",
    "\n",
    "# Between-Group Variance: This portion of the variance captures the variability in the dependent variable due to the differences between the groups defined by the independent variable(s). It reflects the potential effect of the independent variable on the outcome.\n",
    "\n",
    "# Within-Group Variance (Error Variance): This portion represents the variability in the dependent variable within each group. It captures the random error associated with individual data points and any other extraneous factors not accounted for by the independent variable(s).\n",
    "\n",
    "# Partitioning the Variance:\n",
    "\n",
    "# ANOVA uses the concept of sums of squares (SS) to quantify these different sources of variance. The total sum of squares (SST) is partitioned into two components:\n",
    "\n",
    "# Sum of Squares Between Groups (SSB): This reflects the between-group variance and is used to calculate the mean square between groups (MSB).\n",
    "# Sum of Squares Within Groups (SSW): This captures the within-group variance and is used to calculate the mean square within groups (MSW).\n",
    "# Importance of Understanding Partitioning:\n",
    "\n",
    "# Understanding partitioning of variance is important for several reasons:\n",
    "\n",
    "# Assess the Model Fit: The F-statistic in ANOVA is calculated by dividing MSB by MSW. A larger MSB relative to MSW suggests a stronger influence of the independent variable on the dependent variable. This F-statistic is used to test the null hypothesis (no difference between groups) in ANOVA.\n",
    "# Differentiate Signal from Noise: By separating the total variance into its components, we can isolate the variance explained by the independent variable (between-group) from the random error (within-group). This helps us understand how much of the observed variability is actually attributable to the manipulation of the independent variable.\n",
    "# Identify Potential Issues: Unequal variances within groups can affect the validity of ANOVA results. Partitioning allows us to assess this potential issue.\n",
    "# In conclusion, partitioning of variance helps us interpret the results of ANOVA by highlighting the relative contributions of the independent variable and random error to the observed variability in the data. This understanding is essential for drawing sound conclusions about the effects of the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299f3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce59cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 82.5\n",
      "Explained Sum of Squares (SSE): 3.0208333333333366\n",
      "Residual Sum of Squares (SSR): 79.47916666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_way_anova_ss(data, groups):\n",
    "  \"\"\"\n",
    "  Calculates SST, SSE, and SSR for one-way ANOVA.\n",
    "\n",
    "  Args:\n",
    "      data: A 1D NumPy array containing the data values.\n",
    "      groups: A 1D NumPy array with the same length as `data` indicating group membership\n",
    "              for each data point (categorical labels).\n",
    "\n",
    "  Returns:\n",
    "      A dictionary containing:\n",
    "          SST: Total sum of squares.\n",
    "          SSE: Explained sum of squares (between-group).\n",
    "          SSR: Residual sum of squares (within-group).\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate overall mean\n",
    "  overall_mean = np.mean(data)\n",
    "\n",
    "  # Calculate group means (avoiding generator expression)\n",
    "  group_means = []\n",
    "  for group in np.unique(groups):\n",
    "    group_means.append(np.mean(data[groups == group]))\n",
    "  group_means = np.array(group_means)  # Convert to NumPy array\n",
    "\n",
    "  # Calculate total sum of squares\n",
    "  SST = np.sum((data - overall_mean) ** 2)\n",
    "\n",
    "  # Calculate explained sum of squares (between-group)\n",
    "  SSE = np.sum((group_means - overall_mean) ** 2) * len(np.unique(groups))\n",
    "\n",
    "  # Calculate residual sum of squares (within-group)\n",
    "  SSR = SST - SSE\n",
    "\n",
    "  return {\"SST\": SST, \"SSE\": SSE, \"SSR\": SSR}\n",
    "\n",
    "# Example usage\n",
    "data = np.array([10, 8, 12, 14, 9, 11, 15, 13, 7, 6])\n",
    "groups = np.array([\"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"C\", \"C\", \"C\", \"C\"])\n",
    "\n",
    "results = one_way_anova_ss(data, groups)\n",
    "print(\"Total Sum of Squares (SST):\", results[\"SST\"])\n",
    "print(\"Explained Sum of Squares (SSE):\", results[\"SSE\"])\n",
    "print(\"Residual Sum of Squares (SSR):\", results[\"SSR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb912e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "from scipy import stats\n",
    "\n",
    "# Example data (assuming strings representing numbers)\n",
    "data = pd.Series([\"10\", \"8\", \"12\", \"14\", \"9\", \"11\", \"15\", \"13\", \"7\"])\n",
    "row_factors = [\"A\", \"A\", \"B\"]\n",
    "col_factors = [\"Low\", \"Low\", \"High\"]\n",
    "\n",
    "# Convert data to numeric (assuming data contains only numbers)\n",
    "data = pd.to_numeric(data)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"data\": data, \"row_factor\": row_factors, \"col_factor\": col_factors})\n",
    "\n",
    "# Perform two-way ANOVA using pingouin\n",
    "results = pg.anova(dv=\"data\", between=[\"row_factor\", \"col_factor\"], data=df)\n",
    "\n",
    "# Print results (adjust variable names as needed)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4cd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is evidence of a statistically significant difference between the means of the groups.\n",
    "# Here's a breakdown of the interpretation:\n",
    "\n",
    "# F-statistic (5.23): This value represents the ratio of the variance between groups (explained variance) to the variance within groups (unexplained variance). A higher F-statistic indicates a greater relative difference between the group means compared to the variability within each group.\n",
    "# p-value (0.02): This value represents the probability of observing a result this extreme (F-statistic of 5.23 or higher) assuming there are no actual differences between the groups (null hypothesis). A low p-value (typically set at 0.05 or less) suggests we can reject the null hypothesis.\n",
    "# In your case, the p-value of 0.02 is less than the commonly used significance level of 0.05. This suggests that the observed difference between the means is unlikely to be due to random chance. Therefore, we can conclude that at least one group mean is statistically different from the others.\n",
    "\n",
    "# Important Note:\n",
    "\n",
    "# One-way ANOVA only tells you there's a significant difference somewhere between the groups. It doesn't tell you which specific groups are different from each other.\n",
    "# You might need to perform post-hoc tests (e.g., Tukey's HSD test) to identify which specific pairs of groups have statistically significant differences in their means.\n",
    "# Additional Considerations:\n",
    "\n",
    "# It's important to consider the effect size (e.g., partial eta-squared) alongside statistical significance. Even with a significant p-value, the practical importance of the difference between groups might be small depending on the effect size.\n",
    "# One-way ANOVA assumes normality of the data and homogeneity of variances. It's recommended to check these assumptions before interpreting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8135645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26658ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b576a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812004ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d23c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0c4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
